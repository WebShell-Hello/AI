{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_65UXKNZbHD"
   },
   "source": [
    "# Problem 1 - Lunar Lander environment\n",
    "\n",
    "Implement a deep reinforcement learning agent for a game or environment of OpenAI Gym or Gymnasium.\n",
    "\n",
    "Use the lunar_lander environment: https://gymnasium.farama.org/environments/box2d/lunar_lander/.\n",
    "\n",
    "Please plot the learning progress of your method from 0 to 1000 episodes.\n",
    "\n",
    "You can have a figure to show rewards and another figure to show training loss.\n",
    "\n",
    "Please use a video or gifs or figures to demonstrate how your agent works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1DlP4l5cuPc"
   },
   "source": [
    "## Goal\n",
    "\n",
    "The goal is to control a spacecraft as it tries to land safely on the moon's surface while using as little fuel as possible and avoiding being tilted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_j2FTiyZkhf"
   },
   "source": [
    "## State Space (8, ) - Continuous\n",
    "\n",
    "- x coordinates\n",
    "- y coordinates\n",
    "- horizontal speed\n",
    "- vertical speed\n",
    "- angle\n",
    "- how fast it is spinning\n",
    "- whether right landing legs are touching the ground\n",
    "- whether left landing legs are touching the ground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF_oWMLNal8l"
   },
   "source": [
    "## Action Space (4 choices) - Discrete\n",
    "\n",
    "- 0: doing nothing\n",
    "- 1: firing the left engine\n",
    "- 2: firing the right engine\n",
    "- 3: firing the main engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTovR5xrbiRD"
   },
   "source": [
    "## Reward Function\n",
    "\n",
    "The agent receives feedback based on its performance at each timestep:\n",
    "\n",
    "- The reward increases when the lander moves closer to the landing zone, and decreases if it moves farther away.\n",
    "\n",
    "- The reward is increased when the lander is slowing down, and decreased when it moves too fast.\n",
    "\n",
    "- A penalty is given if the lander is tilted, meaning it is not level with the ground.\n",
    "\n",
    "- The agent earns 10 points for each leg that touches the ground.\n",
    "\n",
    "- A small penalty of 0.03 points is given for every time a side engine is used.\n",
    "\n",
    "- A larger penalty of 0.3 points is given for each time the main engine is fired.\n",
    "\n",
    "At the end of the episode, the agent gets a bonus reward of +100 points for landing successfully, or a penalty of -100 points if it crashes.\n",
    "\n",
    "The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "An episode is considered successful if the agent scores at least 200 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBhrn-3m7YH-"
   },
   "source": [
    "## Loss Composition:\n",
    "The total loss used in training the PPO algorithm is calculated as:\n",
    "\n",
    "**totalLoss = PolicyLoss âˆ’ ð‘1 Ã— ValueLoss + ð‘2 Ã— EntropyBonus**\n",
    "\n",
    "This formula represents how the algorithm calculates the error during training and adjusts the policy network accordingly. The total loss is updated periodically over each Rollout cycle, which represents the data collection period. The total loss consists of the following components:\n",
    "\n",
    "**Policy Loss**: This term measures how much the current policy (the agentâ€™s decision-making strategy) differs from the expected or optimal policy. The lower the policy loss, the better the policy.\n",
    "\n",
    "**Value Loss**: This term represents how much the agent's predicted value (the expected future reward from a given state) differs from the actual observed reward. It is related to the accuracy of the value function used to evaluate actions.\n",
    "\n",
    "**Entropy Regularisation Reward (Entropy Bonus)**: This term encourages exploration by rewarding randomness in the agentâ€™s actions. The idea is that the agent should not always stick to the same actions but should try different things to explore new strategies. A higher entropy bonus helps the agent explore more, while a lower one focuses the agent on exploiting its learned strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mankakiu/Documents/Python_AI_Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-15 15:09:05,855] A new study created in memory with name: no-name-45182530-4836-42e2-804e-e126a337c731\n",
      "[I 2025-04-15 15:09:32,421] Trial 0 finished with value: -108.057601 and parameters: {'learning_rate': 0.00017047818897961554, 'n_steps': 256, 'batch_size': 64, 'gamma': 0.9690000000000001, 'gae_lambda': 0.9700000000000001, 'clip_range': 0.4, 'ent_coef': 0.0002774690177819561, 'n_epochs': 15}. Best is trial 0 with value: -108.057601.\n",
      "[I 2025-04-15 15:09:40,412] Trial 1 finished with value: -169.87401560000004 and parameters: {'learning_rate': 0.0010896863351476987, 'n_steps': 512, 'batch_size': 256, 'gamma': 0.9690000000000001, 'gae_lambda': 0.91, 'clip_range': 0.30000000000000004, 'ent_coef': 0.042623367155356, 'n_epochs': 4}. Best is trial 0 with value: -108.057601.\n",
      "[I 2025-04-15 15:10:36,589] Trial 2 finished with value: -119.27409499999999 and parameters: {'learning_rate': 0.008844931923897011, 'n_steps': 512, 'batch_size': 32, 'gamma': 0.972, 'gae_lambda': 0.8700000000000001, 'clip_range': 0.25, 'ent_coef': 0.0014764881603348406, 'n_epochs': 19}. Best is trial 0 with value: -108.057601.\n",
      "[I 2025-04-15 15:10:45,773] Trial 3 finished with value: -415.5559638 and parameters: {'learning_rate': 0.002971619773459456, 'n_steps': 3840, 'batch_size': 128, 'gamma': 0.9480000000000001, 'gae_lambda': 0.92, 'clip_range': 0.35, 'ent_coef': 0.05491952186227, 'n_epochs': 3}. Best is trial 0 with value: -108.057601.\n"
     ]
    }
   ],
   "source": [
    "# 1. Optuna runs 200 trials to find the best hyperparameters for the training process.\n",
    "# 2. The PPO model is trained over 1000 episodes to figure out the best neural network weights (which control how the agent makes decisions), i.e., the correct sequence of actions the agent should take in different situations.\n",
    "\n",
    "# 1 episode is 1 full experiment based on the selected parameters.\n",
    "# A single model might need many steps to complete an episode, but since the environment changes each time, the number of steps required can differ between experiments.\n",
    "# Rewards are calculated over each episode, so the x-axis on the reward graph shows the number of episodes.\n",
    "# Loss is calculated over each step, so the x-axis on the loss graph represents the total number of steps taken across all 1000 episodes, divided by the n_steps parameter from the best hyperparameters.\n",
    "\n",
    "import optuna   # Optuna is used for hyperparameter optimisation\n",
    "import imageio  # Create GIF animations to visualise the agent's performance\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "import gymnasium  # Provide the LunarLander-v3 environment for training\n",
    "from gymnasium.wrappers import RecordVideo  # Record gameplay videos\n",
    "from stable_baselines3 import PPO   # PPO algorithm\n",
    "from stable_baselines3.common.logger import Logger, KVWriter, CSVOutputFormat # Log training data\n",
    "from stable_baselines3.common.monitor import Monitor  # Monitor episode rewards and lengths\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv  # Vectorised environment for parallel training\n",
    "from stable_baselines3.common.evaluation import evaluate_policy   # Evaluate the trained model's performance\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Silently log training results to a file without printing to the console\n",
    "class SilentLogger(Logger):\n",
    "    def __init__(self, log_file: Optional[str] = None):\n",
    "        output_formats = []\n",
    "        if log_file:\n",
    "            output_formats.append(CSVOutputFormat(log_file))  # Set up CSV logging to the specified file\n",
    "        super().__init__(folder=None, output_formats=output_formats)\n",
    "    def record(self, key: str, value, exclude: Optional[KVWriter] = None) -> None:\n",
    "        super().record(key, value, exclude)\n",
    "    def dump(self, step: int = 0) -> None:\n",
    "        # Write logged data to the output file at the given step\n",
    "        super().dump(step)\n",
    "\n",
    "# Track rewards, losses, and control episode count during training\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, total_episodes, verbose):\n",
    "        super().__init__(verbose)\n",
    "        self.total_episodes = total_episodes  # Total number of episodes to train\n",
    "        self.episode_count = 0  # Counter for completed episodes\n",
    "        self.episode_rewards = []  # List to store rewards for each episode\n",
    "        self.current_episode_reward = 0 # Running total reward for current episode\n",
    "        self.total_loss = [] # Store loss values after each rollout\n",
    "        self.policy_loss,self.value_loss,self.entropy = [],[],[] # Lists to store individual components of the loss\n",
    "\n",
    "    # Called at each step to update rewards and check episode completion\n",
    "    def _on_step(self) -> bool:\n",
    "        # Add the sum of rewards from all parallel environments to the current episode total\n",
    "        self.current_episode_reward += sum(self.locals['rewards'])\n",
    "        dones = self.locals.get('dones', [False] * self.model.env.num_envs) # Check which environments are done\n",
    "        for done in dones:\n",
    "            if done:  # If an episode has ended in any environment\n",
    "                self.episode_count += 1 # Increment episode counter\n",
    "                self.episode_rewards.append(self.current_episode_reward)  # Store the completed episode's total rewards\n",
    "                self.current_episode_reward = 0 # Reset reward for next episode\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Episode: {self.episode_count}/{self.total_episodes}\", end=\"\\r\")\n",
    "                if self.episode_count >= self.total_episodes:\n",
    "                    return False  # Stop training when target episodes reached\n",
    "        return True # Continue training\n",
    "\n",
    "    # Called at the end of each rollout to record loss value\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        if hasattr(self.model, 'logger'):\n",
    "            # Retrieve and store the total training loss\n",
    "            loss = self.model.logger.name_to_value.get('train/loss')\n",
    "            if loss is not None: self.total_loss.append(loss) # Append loss to history\n",
    "\n",
    "            # Invidual loss components\n",
    "            # policy_loss = self.model.logger.name_to_value.get('train/policy_gradient_loss')\n",
    "            # if policy_loss is not None: self.policy_loss.append(policy_loss)\n",
    "            # value_loss = self.model.logger.name_to_value.get('train/value_loss')\n",
    "            # if value_loss is not None: self.value_loss.append(value_loss)\n",
    "            # entropy = self.model.logger.name_to_value.get('train/entropy')\n",
    "            # if entropy is not None: self.entropy.append(loss)\n",
    "\n",
    "# Optuna objective function for hyperparameter optimisation\n",
    "# Train a model with sampled hyperparameters and returns average reward\n",
    "def OptunaTrial(trial,model_type,parallel,total_timesteps,n_eval_episodes):\n",
    "    # Define the hyperparameters search space (min, max, steps)\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.00001, 0.01, log=True), # Control how quickly the model adjusts its behaviour based on feedback\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 256, 4096, step=256),               # How many steps the agent takes before updating its strategy and recording the results\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512]), # How many experiences the model will use at a time to update its strategy\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.9, 0.999, step=0.003),             # Control how much the agent values future rewards versus immediate rewards\n",
    "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.8, 0.99, step=0.01),     # Control how much the agent considers just the immediate next step versus looking at the entire series of steps when evaluating its performance\n",
    "        \"clip_range\": trial.suggest_float(\"clip_range\", 0.1, 0.4, step = 0.05),       # Control how much the agent is allowed to change its strategy during each update\n",
    "        \"ent_coef\": trial.suggest_float(\"ent_coef\", 0.0001, 0.1, log=True),           # Control how much randomness the agent should have in its decisions\n",
    "        \"n_epochs\": trial.suggest_int(\"n_epochs\", 3, 20)                            # How many times the agent will review its experience and update its strategy during training\n",
    "    }\n",
    "\n",
    "    # Create a vectorised environment with 8 parallel instances of LunarLander-v3\n",
    "    env = DummyVecEnv([lambda: Monitor(gymnasium.make(model_type)) for _ in range(parallel)])\n",
    "    # Initialise PPO model with the sampled hyperparameters\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0, **hyperparams)\n",
    "    try:\n",
    "        # Train the model for 100,000 timesteps\n",
    "        model.learn(total_timesteps=total_timesteps)\n",
    "        # Evaluate the trained model over 10 episodes\n",
    "        mean_reward, _ = evaluate_policy(model=model, env=env, n_eval_episodes=n_eval_episodes)\n",
    "        # Report the mean reward to Optuna for optimisation\n",
    "        trial.report(mean_reward, step=0)\n",
    "        if trial.should_prune():  # If trial is underperforming, prune it\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    finally:\n",
    "        env.close() # Clean up by closing the environment\n",
    "    return mean_reward  # Return the mean reward as the optimisation target\n",
    "\n",
    "# Train the PPO agent with the best hyperparameters found by Optuna\n",
    "def TrainPpoAgent(best_params, model_type, model_name, total_episodes, total_timesteps, parallel,n_eval_episodes):\n",
    "    # Create a single environment and enable human rendering mode\n",
    "    # setting up one instance of the environment and turning on the visual display so a human can watch whatâ€™s happening (like seeing the game play out on the screen).\n",
    "    # env = Monitor(gym.make(\"LunarLander-v3\", render_mode=\"human\"))\n",
    "\n",
    "    # Create a vectorised environment with specified number of parallel instances\n",
    "    env = DummyVecEnv([lambda: Monitor(gymnasium.make(model_type)) for _ in range(parallel)])\n",
    "    # Initialise PPO model with best hyperparameters\n",
    "    model = PPO(policy = \"MlpPolicy\", env=env, verbose=1, **best_params)  # verbose=1 enables logging\n",
    "\n",
    "    # Set up a silent logger to save training data to a CSV file\n",
    "    logger = SilentLogger(log_file=f\"{model_name}_training_log.csv\")\n",
    "    model.set_logger(logger)\n",
    "    # Initialise callback to track training progress\n",
    "    callback = TrainingCallback(total_episodes=total_episodes, verbose=1)\n",
    "    print(f\"tarting training until {total_episodes} episodes...\")\n",
    "    # Train the model, limiting to 10M timesteps, and use callback to monitor\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    model.save(model_name)  # Save the trained model\n",
    "    # Evaluate the model over 10 episodes\n",
    "    mean_reward, std_reward = evaluate_policy(model=model, env=env, n_eval_episodes=n_eval_episodes)\n",
    "    print(f\"Mean Reward = {mean_reward}, Std Reward = {std_reward}\")\n",
    "    env.close() # Clean up by closing the environment\n",
    "    return model, callback  # Return the model and callback data\n",
    "\n",
    "# Record a GIF of the agent's performance\n",
    "def recordGif(id,model, filename, fps, duration, random_seed):\n",
    "    # Create a monitored environment with RGB rendering\n",
    "    with Monitor(gymnasium.make(id=id, render_mode=\"rgb_array\")) as env:\n",
    "        frames = [] # List to store rendered frames\n",
    "        observation, _ = env.reset(seed = random_seed)  # Reset environment with seed\n",
    "        for _ in range(fps * duration): # Run for specified duration\n",
    "            frame = env.render()  # Render the current state\n",
    "            if frame is None:\n",
    "                raise ValueError(\"Environment render() returned None. Check render_mode.\")\n",
    "            frames.append(frame)  # Store the frame\n",
    "            # Predict the next action deterministically\n",
    "            action, _ = model.predict(observation=observation, deterministic=True)\n",
    "            # Step the environment with the action\n",
    "            observation, _, done, _, _ = env.step(action)\n",
    "            if done:  # If episode ends, reset the environment\n",
    "                observation, _ = env.reset()\n",
    "        # Save frames as a GIF\n",
    "        imageio.mimsave(uri=filename, ims=frames, fps=fps)\n",
    "        print(f\"GIF saved to {filename}\")\n",
    "\n",
    "# Record a video of the agent's performance\n",
    "def recordVideo(id,model, video_name, random_seed):\n",
    "    # Create an environment with video recording\n",
    "    env = RecordVideo(\n",
    "        Monitor(gymnasium.make(id= id, render_mode=\"rgb_array\")),\n",
    "        video_folder=\".\",\n",
    "        name_prefix=video_name,\n",
    "        episode_trigger=lambda x: True  # Record every episode\n",
    "    )\n",
    "    with env:\n",
    "        observation, _ = env.reset(seed = random_seed)  # Reset environment with seed\n",
    "        while True:\n",
    "            # Predict the next action deterministically\n",
    "            action, _ = model.predict(observation=observation, deterministic=True)\n",
    "            # Step the environment and check for termination\n",
    "            observation, _, done, truncated, _ = env.step(action)\n",
    "            if done or truncated: # Stop if episode ends or is truncated\n",
    "                break\n",
    "    print(f\"Video saved to {video_name}\")\n",
    "\n",
    "# Plot training curves for rewards and losses\n",
    "def plotTrainingCurves(callback, model_name):\n",
    "    plt.figure(figsize=(10, 16))\n",
    "    # Plot episode rewards\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(callback.episode_rewards, label=\"Episode Reward\")\n",
    "    plt.title('Episode Rewards (0 to 1000 Episodes)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Episode Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot total loss. Assume each episode has about 300 steps on average\n",
    "    # (LunarLander-v3 episodes typically range from 100 to 400 steps).\n",
    "    # 8 - parallel environment, 1000 episodes\n",
    "    # Rollout = 300 * 1000 / (8 * n_steps)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(callback.total_loss, label=\"Total Loss\")\n",
    "    plt.title('Total Training Loss')\n",
    "    plt.xlabel('Rollout')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_training_curves.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"The training curve has been saved to {model_name}_training_curves.png\")\n",
    "\n",
    "def main():\n",
    "    model_type= \"LunarLander-v3\"\n",
    "    # Use Optuna for hyperparameter tuning. It uses probability modeling to guess which combinations of parameters\n",
    "    # are more likely to give better rewards, and then focuses on trying those combinations first\n",
    "\n",
    "    # Default optimal parameters can be found at:\n",
    "    # https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/ppo/ppo.html#PPO\n",
    "\n",
    "    # # Example of best parameters found by Optuna:\n",
    "    # best_params = {\n",
    "    #     'learning_rate': 0.001498919063649211,\n",
    "    #     'n_steps': 1024,\n",
    "    #     'batch_size': 256,\n",
    "    #     'gamma': 0.993,\n",
    "    #     'gae_lambda': 0.9600000000000001,\n",
    "    #     'clip_range': 0.25,\n",
    "    #     'ent_coef': 0.020190545985733898,\n",
    "    #     'n_epochs': 19\n",
    "    # }\n",
    "\n",
    "    # Create an Optuna stude to maximise reward\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    # Run 200 trials to find the best hyperparameters\n",
    "    # study.optimize(OptunaTrial, n_trials=200)\n",
    "    study.optimize(partial(OptunaTrial,model_type=model_type,parallel=8,total_timesteps=100_000,n_eval_episodes=10), n_trials=200)\n",
    "\n",
    "    # Print the best hyperparameters and results after optimisation\n",
    "    print(\"Best hyperparameters:\", study.best_params\n",
    "          , \"\\nBest accuracy:\", study.best_value\n",
    "          , \"\\nBest step:\", study.best_params['n_steps'])\n",
    "\n",
    "    base_name = f\"{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}_ppo_lunarlander\"\n",
    "\n",
    "    # Save Optuna trial results to a CSV file\n",
    "    trials_df = study.trials_dataframe()\n",
    "    completed_trials = trials_df[trials_df['state'] == 'COMPLETE']   # Filter completed trials\n",
    "    completed_trials.to_csv(f\"{base_name}_optuna_log.csv\", index=False)\n",
    "\n",
    "    model_name = base_name\n",
    "    # Train the PPO agent with the best parameters\n",
    "    model, callback = TrainPpoAgent(best_params=study.best_params,model_type=model_type, model_name=model_name,total_episodes=1000,total_timesteps=10_000_000,parallel=8,n_eval_episodes=10)\n",
    "\n",
    "    random_seed = 42  # Set a random seed to make sure the environment is the same each time when generating gifs and videos\n",
    "    # Generate visualisation outputs (GIF, Video, Plots)\n",
    "    recordGif(id=model_type, model=model, filename=f\"{base_name}.gif\", fps=30, duration=10, random_seed=random_seed)\n",
    "    recordVideo(id=model_type, model=model, video_name=base_name,random_seed=random_seed)\n",
    "    plotTrainingCurves(callback, model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
